from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoModelForSemanticSegmentation, AutoImageProcessor
from PIL import Image, ImageDraw, ImageFont, ImageEnhance, ImageFilter
import torch
import torch.nn.functional as F
import numpy as np
import cv2
import io
import os
import uvicorn
import base64

# ===== FastAPI ì•± ìƒì„± =====
app = FastAPI(title="Smart Recycling Segmentation API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {
        "message": "Smart Recycling Segmentation API is running!",
        "version": "3.0",
        "features": ["adaptive_background_removal", "multi_class_separation", "depth_based_isolation"]
    }

@app.get("/health")
async def health():
    return {"status": "healthy", "gpu_available": torch.cuda.is_available()}

# ===== ëª¨ë¸ ë¡œë“œ =====
MODEL_PATH = os.path.abspath(os.path.dirname(__file__))

try:
    print("ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = AutoModelForSemanticSegmentation.from_pretrained(MODEL_PATH, local_files_only=True)
    processor = AutoImageProcessor.from_pretrained(MODEL_PATH, local_files_only=True)
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    model = None
    processor = None

# ===== ì„¤ì • =====
class_names = ["background", "can", "glass", "paper", "plastic", "styrofoam", "vinyl"]
class_colors_bright = [
    None,             # background - ìƒ‰ìƒ ì—†ìŒ (íˆ¬ëª…)
    (255, 69, 0),     # can - ì£¼í™©ë¹¨ê°•
    (50, 205, 50),    # glass - ë¼ì„ê·¸ë¦°
    (30, 144, 255),   # paper - íŒŒë‘
    (255, 20, 147),   # plastic - ë”¥í•‘í¬
    (255, 215, 0),    # styrofoam - ê³¨ë“œ
    (138, 43, 226)    # vinyl - ë³´ë¼
]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")

font_path = os.path.join(os.path.dirname(__file__), "Pretendard-SemiBold.otf")

# ===== ë°°ê²½ ì œê±° ë° ê°ì²´ ë¶„ë¦¬ í•¨ìˆ˜ë“¤ =====

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ë“¤ ì œê±° (ì™„ì „ ì‚­ì œ)
# ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ë“¤:
# - smart_background_removal (ì œê±°ë¨)
# - separate_complex_objects (ì œê±°ë¨) 
# - enhance_segmentation_quality (ì œê±°ë¨)
# - remove_noise_and_smooth (ì œê±°ë¨)

# ëª¨ë“  KMeans ê´€ë ¨ importë„ ì œê±° (ë” ì´ìƒ ì‚¬ìš© ì•ˆ í•¨)

# ===== ì‹œê°í™” í•¨ìˆ˜ë“¤ =====

def create_enhanced_visualization(image, mask, object_mask=None):
    """í–¥ìƒëœ ì‹œê°í™” ìƒì„± - ì˜¬ë°”ë¥¸ Background ì²˜ë¦¬"""
    print("ğŸ¨ í–¥ìƒëœ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    img_np = np.array(image)
    h, w = img_np.shape[:2]
    
    # === OVERLAY ì´ë¯¸ì§€: Background íˆ¬ëª…(ì›ë³¸), ê°ì²´ ì»¬ëŸ¬ ===
    overlay = img_np.copy().astype(np.float32)
    
    for class_id in range(1, len(class_names)):  # Background(0) ì œì™¸
        class_region = (mask == class_id)
        if np.any(class_region):
            color = class_colors_bright[class_id]
            # ì›ë³¸ ì´ë¯¸ì§€ì— ì»¬ëŸ¬ ì˜¤ë²„ë ˆì´
            overlay[class_region] = (
                img_np[class_region].astype(np.float32) * 0.4 +  # ì›ë³¸ 40%
                np.array(color) * 0.6  # ì»¬ëŸ¬ 60%
            )
    
    overlay = np.clip(overlay, 0, 255).astype(np.uint8)
    
    # === PREDICT ì´ë¯¸ì§€: Background ê²€ì€ìƒ‰, ê°ì²´ ë°ì€ ì»¬ëŸ¬ ===
    predict = np.zeros_like(img_np)  # ê²€ì€ìƒ‰ ë°°ê²½
    
    for class_id in range(1, len(class_names)):  # Background(0) ì œì™¸
        class_region = (mask == class_id)
        if np.any(class_region):
            color = class_colors_bright[class_id]
            predict[class_region] = color  # ìˆœìˆ˜ ì»¬ëŸ¬
    
    # ë¼ë²¨ ì¶”ê°€ (ê°ì²´ì—ë§Œ)
    overlay_pil = add_clean_labels(Image.fromarray(overlay), mask)
    predict_pil = add_clean_labels(Image.fromarray(predict), mask)
    
    return predict_pil, overlay_pil

def add_clean_labels(image, mask):
    """ê¹”ë”í•œ ë¼ë²¨ ì¶”ê°€ - Background ì œì™¸"""
    draw = ImageDraw.Draw(image)
    try:
        font = ImageFont.truetype(font_path, 18)
    except:
        try:
            font = ImageFont.load_default()
        except:
            return image

    for class_id in range(1, len(class_names)):  # Background(0) ì œì™¸
        class_mask = (mask == class_id)
        if not np.any(class_mask):
            continue

        # í´ë˜ìŠ¤ ì˜ì—­ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
        y_coords, x_coords = np.where(class_mask)
        if len(x_coords) == 0 or len(y_coords) == 0:
            continue
            
        x_center = int(np.mean(x_coords))
        y_center = int(np.mean(y_coords))
        
        label = class_names[class_id]
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        try:
            bbox = draw.textbbox((0, 0), label, font=font)
            text_w, text_h = bbox[2] - bbox[0], bbox[3] - bbox[1]
        except:
            text_w, text_h = len(label) * 8, 12
        
        padding = 6
        
        # ë°°ê²½ ë°•ìŠ¤ (í…Œë‘ë¦¬ ì—†ìŒ)
        box = [
            x_center - text_w//2 - padding,
            y_center - text_h//2 - padding,
            x_center + text_w//2 + padding,
            y_center + text_h//2 + padding
        ]
        
        # ë°˜íˆ¬ëª… ê²€ì€ ë°°ê²½
        draw.rectangle(box, fill=(0, 0, 0, 180))
        
        # í°ìƒ‰ í…ìŠ¤íŠ¸
        draw.text((x_center - text_w//2, y_center - text_h//2), 
                 label, fill="white", font=font)
        
    return image

# ===== ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜ =====

def process_smart_segmentation(image_bytes):
    """ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬"""
    if model is None or processor is None:
        raise HTTPException(status_code=500, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    try:
        print("ğŸš€ ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì‹œì‘...")
        
        # 1. ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        img_np = np.array(image)
        print(f"   ì›ë³¸ í¬ê¸°: {image.size}")
        
        # 2. ìŠ¤ë§ˆíŠ¸ ë°°ê²½ ì œê±°
        object_mask = smart_background_removal(img_np)
        
        # 3. ëª¨ë¸ ì…ë ¥ ì¤€ë¹„ ë° ì…ë ¥ ì´ë¯¸ì§€ ë””ë²„ê¹…
        print(f"ğŸ“‹ PIL ì´ë¯¸ì§€ í†µê³„:")
        img_array = np.array(image)
        print(f"   í¬ê¸°: {img_array.shape}")
        print(f"   í”½ì…€ ë²”ìœ„: {img_array.min()} ~ {img_array.max()}")
        print(f"   í‰ê· : {img_array.mean():.2f}")
        
        # ì…ë ¥ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì–´ë‘ìš´ ê²½ìš° ë°ê¸° ì¡°ì •
        if img_array.mean() < 50:
            print("âš ï¸ ì…ë ¥ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì–´ë‘¡ìŠµë‹ˆë‹¤. ë°ê¸° ì¡°ì •...")
            from PIL import ImageEnhance
            enhancer = ImageEnhance.Brightness(image)
            image = enhancer.enhance(2.0)  # 2ë°° ë°ê²Œ
            img_array = np.array(image)
            print(f"   ì¡°ì • í›„ í”½ì…€ ë²”ìœ„: {img_array.min()} ~ {img_array.max()}")
            print(f"   ì¡°ì • í›„ í‰ê· : {img_array.mean():.2f}")
        
        inputs = processor(images=image, return_tensors="pt")
        print(f"ğŸ“‹ ì…ë ¥ í…ì„œ í¬ê¸°: {inputs['pixel_values'].shape}")
        print(f"ğŸ“‹ ì…ë ¥ í…ì„œ ë²”ìœ„: {inputs['pixel_values'].min():.3f} ~ {inputs['pixel_values'].max():.3f}")
        
        # ì…ë ¥ í…ì„œê°€ ëª¨ë‘ 0ì¸ ê²½ìš° ê°•ì œë¡œ ì •ê·œí™”
        if inputs['pixel_values'].max() == 0:
            print("âš ï¸ ì…ë ¥ í…ì„œê°€ ëª¨ë‘ 0ì…ë‹ˆë‹¤. ì§ì ‘ ì •ê·œí™” ì‹œë„...")
            # ImageNet ì •ê·œí™” ì—­ì‚°
            img_tensor = torch.tensor(img_array.transpose(2, 0, 1), dtype=torch.float32) / 255.0
            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
            normalized = (img_tensor - mean) / std
            inputs['pixel_values'] = normalized.unsqueeze(0)
            print(f"   ì§ì ‘ ì •ê·œí™” í›„ ë²”ìœ„: {inputs['pixel_values'].min():.3f} ~ {inputs['pixel_values'].max():.3f}")
        
        # 4. ëª¨ë¸ ì˜ˆì¸¡ (ë” ì •ë°€í•˜ê²Œ)
        with torch.no_grad():
            outputs = model(pixel_values=inputs["pixel_values"].to(device))
            logits = outputs.logits
            print(f"ğŸ“‹ ì¶œë ¥ ë¡œì§“ í¬ê¸°: {logits.shape}")
            print(f"ğŸ“‹ ì¶œë ¥ ë¡œì§“ ë²”ìœ„: {logits.min():.3f} ~ {logits.max():.3f}")
            
            # ë” ë†’ì€ í•´ìƒë„ë¡œ ì—…ìƒ˜í”Œë§
            target_size = (image.size[1], image.size[0])  # (H, W)
            logits = F.interpolate(logits, size=target_size, mode="bilinear", align_corners=False)
            
            probs = F.softmax(logits, dim=1)[0].cpu().numpy()
            print(f"ğŸ“‹ í™•ë¥  ë¶„í¬ - ê° í´ë˜ìŠ¤ë³„ ìµœëŒ€ê°’:")
            for i in range(len(class_names)):
                max_prob = probs[i].max()
                mean_prob = probs[i].mean()
                print(f"   {class_names[i]}: max={max_prob:.3f}, mean={mean_prob:.3f}")
            
            initial_mask = np.argmax(probs, axis=0)
            print(f"ğŸ“‹ ëª¨ë¸ ì›ì‹œ ì˜ˆì¸¡: {np.unique(initial_mask, return_counts=True)}")
            
            # ë” ê´€ëŒ€í•œ í™•ì‹ ë„ ê¸°ë°˜ í•„í„°ë§
            confidence_threshold = 0.05  # ë” ë‚®ì¶¤
            max_probs = np.max(probs, axis=0)
            low_confidence = max_probs < confidence_threshold
            initial_mask[low_confidence] = 0
            
            print(f"ğŸ“‹ í™•ì‹ ë„ í•„í„°ë§ í›„: {np.unique(initial_mask, return_counts=True)}")
            
            # ë§Œì•½ ì—¬ì „íˆ ëª¨ë“  ê²Œ ë°°ê²½ì´ë©´ ê°•ì œë¡œ ê°€ì¥ ë†’ì€ í™•ë¥  ì˜ì—­ì„ í´ë˜ìŠ¤ë¡œ í• ë‹¹
            if len(np.unique(initial_mask)) == 1 and np.unique(initial_mask)[0] == 0:
                print("âš ï¸ ëª¨ë“  í”½ì…€ì´ ë°°ê²½ìœ¼ë¡œ ë¶„ë¥˜ë¨, ë” ì ê·¹ì  ê°•ì œ í• ë‹¹...")
                
                # ê° í´ë˜ìŠ¤ë³„ë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í”½ì…€ë“¤ ì°¾ê¸°
                for class_id in range(1, len(class_names)):
                    class_probs = probs[class_id]
                    if class_probs.max() > 0.001:  # ë§¤ìš° ë§¤ìš° ë‚®ì€ ì„ê³„ê°’
                        # ìƒìœ„ 5% í”½ì…€ì„ í•´ë‹¹ í´ë˜ìŠ¤ë¡œ í• ë‹¹
                        threshold = np.percentile(class_probs, 95)
                        high_prob_mask = class_probs >= threshold
                        if high_prob_mask.sum() > 50:  # ìµœì†Œ 50í”½ì…€
                            initial_mask[high_prob_mask] = class_id
                            print(f"   ê°•ì œ í• ë‹¹: {class_names[class_id]} - {high_prob_mask.sum()}í”½ì…€")
                            
                print(f"ğŸ“‹ ê°•ì œ í• ë‹¹ í›„: {np.unique(initial_mask, return_counts=True)}")
        
        print("   ë°°ê²½ ë§ˆìŠ¤í¬ ì ìš© ìƒëµ (ë””ë²„ê¹… ëª¨ë“œ)")
        
        # 7. ë³µí•© ê°ì²´ ë¶„ë¦¬
        separated_mask = separate_complex_objects(initial_mask, probs)
        
        # 8. ì„¸ê·¸ë©˜í…Œì´ì…˜ í’ˆì§ˆ í–¥ìƒ
        enhanced_mask = enhance_segmentation_quality(img_np, separated_mask, probs)
        
        # 9. ë…¸ì´ì¦ˆ ì œê±° ë° ìµœì¢… ì •ì œ
        final_mask = remove_noise_and_smooth(enhanced_mask)
        
        # 10. ê²°ê³¼ ë¶„ì„ - ë” ê´€ëŒ€í•œ ê¸°ì¤€
        unique, counts = np.unique(final_mask, return_counts=True)
        print("ğŸ“Š ìµœì¢… ê²°ê³¼:")
        detected_classes = []
        total_pixels = final_mask.size
        
        for u, c in zip(unique, counts):
            if u > 0 and u < len(class_names):  # ìµœì†Œ í”½ì…€ ìˆ˜ ì¡°ê±´ ì œê±°
                percentage = (c / total_pixels) * 100
                print(f"   {class_names[u]}: {c:,}px ({percentage:.1f}%)")
                if c > 50:  # 50í”½ì…€ ì´ìƒë§Œ ìœ íš¨í•œ í´ë˜ìŠ¤ë¡œ ì¸ì •
                    detected_classes.append((class_names[u], c))
        
        # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
        detected_classes.sort(key=lambda x: x[1], reverse=True)
        class_names_only = [cls[0] for cls in detected_classes]
        
        print(f"ğŸ“‹ ê°ì§€ëœ í´ë˜ìŠ¤ë“¤: {class_names_only}")
        
        # ì•„ë¬´ê²ƒë„ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš° ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        if not class_names_only:
            print("âš ï¸ ì•„ë¬´ê²ƒë„ ê°ì§€ë˜ì§€ ì•ŠìŒ!")
            print(f"   ì´ˆê¸° ë§ˆìŠ¤í¬ ìœ ë‹ˆí¬ ê°’: {np.unique(initial_mask)}")
            print(f"   ë¶„ë¦¬ í›„ ë§ˆìŠ¤í¬ ìœ ë‹ˆí¬ ê°’: {np.unique(separated_mask)}")
            print(f"   í–¥ìƒ í›„ ë§ˆìŠ¤í¬ ìœ ë‹ˆí¬ ê°’: {np.unique(enhanced_mask)}")
            print(f"   ìµœì¢… ë§ˆìŠ¤í¬ ìœ ë‹ˆí¬ ê°’: {np.unique(final_mask)}")
            
            # ê°•ì œë¡œ ìµœì†Œí•œì˜ ê²°ê³¼ë¼ë„ ë°˜í™˜
            if np.any(initial_mask > 0):
                fallback_unique, fallback_counts = np.unique(initial_mask, return_counts=True)
                for u, c in zip(fallback_unique, fallback_counts):
                    if u > 0 and u < len(class_names) and c > 10:
                        class_names_only.append(class_names[u])
                        print(f"   í´ë°±ìœ¼ë¡œ ì¶”ê°€: {class_names[u]} ({c}px)")
                        break
        
        # 11. ì‹œê°í™” ìƒì„±
        pred_img, overlay_img = create_enhanced_visualization(image, final_mask, object_mask)
        
        print("âœ… ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ!")
        return pred_img, overlay_img, class_names_only

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

# ===== FastAPI ì—”ë“œí¬ì¸íŠ¸ =====

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰"""
    try:
        if not file.content_type.startswith("image/"):
            raise HTTPException(status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤")

        print(f"ğŸ“¤ ìš”ì²­ ë°›ìŒ: {file.filename} ({file.content_type})")
        
        image_bytes = await file.read()
        print(f"   íŒŒì¼ í¬ê¸°: {len(image_bytes):,} bytes")
        
        # ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì²˜ë¦¬
        pred_img, overlay_img, detected_classes = process_smart_segmentation(image_bytes)

        # ê²°ê³¼ ì¸ì½”ë”©
        pred_bytes = io.BytesIO()
        overlay_bytes = io.BytesIO()
        pred_img.save(pred_bytes, format="PNG", optimize=True, quality=90)
        overlay_img.save(overlay_bytes, format="PNG", optimize=True, quality=90)

        # ì£¼ìš” í´ë˜ìŠ¤ ê²°ì • (ê°€ì¥ í° ì˜ì—­)
        main_class = detected_classes[0] if detected_classes else "unknown"
        confidence = min(0.95, max(0.7, len(detected_classes) * 0.15 + 0.7))

        response = {
            "prediction": base64.b64encode(pred_bytes.getvalue()).decode("utf-8"),
            "overlay": base64.b64encode(overlay_bytes.getvalue()).decode("utf-8"),
            "class": main_class,
            "confidence": confidence,
            "detected_classes": detected_classes,
            "status": "success",
            "message": "ìŠ¤ë§ˆíŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì™„ë£Œ"
        }
        
        print(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡: {main_class} ({confidence:.2f}) - {detected_classes}")
        return response

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/predict-raw")
async def predict_raw(file: UploadFile = File(...)):
    """ì›ë³¸ ì´ë¯¸ì§€ë„ ì²˜ë¦¬ ê°€ëŠ¥ (í˜¸í™˜ì„±ìš©)"""
    return await predict(file)

# ===== ì„œë²„ ì‹¤í–‰ =====

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    print(f"ğŸš€ ì„œë²„ ì‹œì‘: http://0.0.0.0:{port}")
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=False)
